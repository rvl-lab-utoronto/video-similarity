<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:site_name" content="SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos">
    <meta property="og:title" content="SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos">
    <meta property="og:description" content="test">
    <meta property="og:url" content="https://rvl.cs.toronto.edu/video-similarity">
    <meta property="og:image" content="https://rvl.cs.toronto.edu/video-similarity/teaser.jpg">
    <meta property="og:image:type" content="image/jpeg">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Learning To Search in Task and Motion Planning with Streams">
    <meta name="twitter:description" content="Learning to predict the relevance of optimistic quantities in a Task and Motion Planning Problem.">
    <meta name="twitter:creator" content="@eric_heiden">
    <meta name="twitter:url" content="https://rvl.cs.toronto.edu/video-similarity">
    <meta name="twitter:image:src" content="https://rvl.cs.toronto.edu/video-similarity/teaser.jpg">

    <meta name="description" content="Learning to predict the relevance of optimistic quantities in a Task and Motion Planning Problem.">
    <meta name="author" content="Mohamed Khodeir, Ben Agro, Florian Shkurti">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6" crossorigin="anonymous">


    <link rel="icon" type="image/png" href="https://rvl.cs.toronto.edu/video-similarity/favicon.ico" />
    <link rel="icon" type="image/png" href="https://rvl.cs.toronto.edu/video-similarity/favicon.ico" />
    <title>Learning To Search in Task and Motion Planning with Streams</title>

    <style>
        div.authors p:first-of-type {
            font-size: 1.4em;
        }
        
        div.authors p {
            text-align: center;
        }
        
        div.affiliations p {
            text-align: center;
        }
        
        h1,
        h2 {
            margin: 1em 0;
            text-align: center;
        }
        
        a:link {
            text-decoration: none;
        }
        
        #video {
            text-align: center;
        }
        
        div.container {
            padding-top: 60px;
        }
        
        .navbar {
            padding: 0.4em 2em;
        }
        
        #abstract p {
            text-align: justify;
        }
        
        #results .label {
            margin-top: 1em;
            margin-bottom: 0.3em;
            font-weight: bold;
        }
        
        #results h3,
        #results h5 {
            margin-top: 2em;
        }
        
        .btn {
            width: 75%;
        }
        
        .actions .col {
            text-align: center;
        }
    </style>
</head>

<body>
    <nav class="navbar navbar-expand-md navbar-light fixed-top bg-light">
        <!-- <a class="navbar-brand" href="#">Learning To Search in Task and Motion Planning with Streams</a> -->

        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarToggle">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse justify-content-end position-relative" id="navbarToggle">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="#" target="_self">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#abstract" target="_self">Abstract</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#video" target="_self">Video</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" href="#results" target="_self">Results</a>
                </li>
            </ul>
        </div>
    </nav>

    <div class="container">

        <h1>SLIC: Self-Supervised Learning with Iterative Clustering for Human Action Videos</h1>

        <div class="row authors">
            <div class="col">
                <p>
                    <a href=""></a>Salar Hosseini Khorasgani<sup>*</sup>
                </p>
            </div>
            <div class="col">
                <p>
                    <a href="https://sherrychen127.github.io/">Yuxuan Chen</a><sup>*</sup>
                </p>
            </div>
            <div class="col">
                <p>
                    <a href="https://www.cs.toronto.edu/~florian/">Florian Shkurti</a><sup></sup>
                </p>
            </div>
        </div>
        <div class="row affiliations">
            <!-- <div class="col">
                <p><sup>1</sup>Robot Vision and Learning Lab</p>
            </div> -->
            <div class="col">
                <p>University of Toronto Robotics Institute</p>
            </div>
            <div class="col">
                <p><sup>*</sup>equal contributions</p>
            </div>

        </div>
    </div>

    <div class="container" id="abstract">
        <div class="row">
            <h2>Abstract</h2>
            <!-- <p class="col-sm-10 fs-5"> -->
            <p class="col">
                Self-supervised methods have significantly closed the gap with end-to-end supervised learning for image classification~\cite{byol, chen2020simple}. In the case of human action videos, however, where both appearance and motion are significant factors of
                variation, this gap remains significant~\cite{coclr, xie2018rethinking}. One of the key reasons for this is that sampling pairs of similar video clips, a required step for many self-supervised contrastive learning methods, is currently
                done conservatively to avoid false positives. A typical assumption is that similar clips only occur temporally close within a single video, leading to insufficient examples of motion similarity. To mitigate this, we propose SLIC, a clustering-based
                self-supervised contrastive learning method for human action videos. Our key contribution is that we improve upon the traditional intra-video positive sampling by using iterative clustering to group similar video instances. This enables
                our method to leverage pseudo-labels from the cluster assignments to sample harder positives and negatives. SLIC outperforms state-of-the-art video retrieval baselines by +15.4\% on top-1 recall on UCF101 and by +5.7\% when directly transferred
                to HMDB51. With end-to-end finetuning for action classification, SLIC achieves 83.2\% top-1 accuracy (+0.8\%) on UCF101 and 54.5\% on HMDB51 (+1.6\%). SLIC is also competitive with the state-of-the-art in action classification after self-supervised
                pretraining on Kinetics400.
            </p>
        </div>

        <div class="row actions">
            <div class="col"><a href="https://arxiv.org/abs/2111.13144" target="_blank" class="btn btn-primary ">Paper</a></div>
            <div class="col"><a href="https://github.com/rvl-lab-utoronto/drake-tamp" class="btn btn-primary col">Code</a></div>
            <div class="col"><a href="#" data-bs-toggle="modal" data-bs-target="#citationModal" class="btn btn-primary col">Citation</a></div>
        </div>
    </div>

    <div class="modal fade" id="citationModal" tabindex="-1" role="dialog" aria-labelledby="citationModalTitle">
        <div class="modal-dialog modal-dialog-centered" role="document">
            <div class="modal-content">
                <div class="modal-header">
                    <h5 class="modal-title" id="citationModalLongTitle">Citation</h5>
                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                </div>
                <div class="modal-body">
                    <pre>
@misc{khodeir2021learning,
      title={Learning to Search in Task and Motion Planning with Streams}, 
      author={Mohamed Khodeir and Ben Agro and Florian Shkurti},
      year={2021},
      eprint={2111.13144},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}
                    </pre>
                </div>
            </div>
        </div>
    </div>


    <div class="container" id="video">
        <h2>Video</h2>
        <div class="row justify-content-md-center">
            <div class="ratio ratio-16x9 w-75">
                <iframe src="https://www.youtube.com/embed/pzzpR4wh_Zk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
        </div>
    </div>

    <div class="container" id="results">
        <h2>Additional Results</h2>


    </div>


    <!--     <div class="bg-light">
        <div class="container" align="right" style="padding:40px 0">
            Last updated on July 19, 2021
        </div>
    </div> -->

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js" integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf" crossorigin="anonymous"></script>
</body>

</html>